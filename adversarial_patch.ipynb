{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea3338e-fd0f-4ca9-98c7-18c27f981f9a",
   "metadata": {},
   "source": [
    "# ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dff16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yo46/anaconda3/envs/llmattack/lib/python3.8/site-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /home/yo46/.cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/releases/download/resnet/cifar10_resnet20-4118986f.pt\" to /home/yo46/.cache/torch/hub/checkpoints/cifar10_resnet20-4118986f.pt\n",
      "100%|██████████| 1.09M/1.09M [00:00<00:00, 13.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Transform for the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and test datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Load model and move to device\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(net, dataloader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# # Evaluate on the training data\n",
    "# train_accuracy = calculate_accuracy(model, trainloader, device)\n",
    "# print('Accuracy of the network on the training images: %d %%' % train_accuracy)\n",
    "\n",
    "# Evaluate on the test data\n",
    "test_accuracy = calculate_accuracy(model, testloader, device)\n",
    "print('Accuracy of the network on the test images: %d %%' % test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8025fc-c58f-4099-a443-27cc4b5114af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Average Loss: 7.7600\n"
     ]
    }
   ],
   "source": [
    "def create_initial_patch(size=(10, 10), device='cpu'):\n",
    "    \"\"\"Create an initial random patch.\"\"\"\n",
    "    patch = torch.rand((3,) + size, requires_grad=True, device=device)  # Add channel dimension\n",
    "    return patch\n",
    "\n",
    "def apply_patch(original_images, patch):\n",
    "    \"\"\"Apply the patch to the top-left corner of each image in a batch.\"\"\"\n",
    "    patched_images = original_images.clone()\n",
    "    c, h, w = patch.shape  # Get the shape of the patch\n",
    "    patched_images[:, :, :h, :w] = patch  # Apply the patch\n",
    "    return patched_images\n",
    "\n",
    "def train_patch(model, patch, train_loader, device='cpu', epochs=5):\n",
    "    \"\"\"Train the patch to perform an untargeted attack.\"\"\"\n",
    "    optimizer = torch.optim.Adam([patch], lr=0.01)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            patched_images = apply_patch(images, patch)\n",
    "\n",
    "            outputs = model(patched_images)\n",
    "            loss = -torch.nn.functional.nll_loss(outputs, outputs.max(1)[1])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        average_loss = total_loss / num_batches\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    return patch\n",
    "\n",
    "\n",
    "# Load your datasets and model here\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create the initial patch\n",
    "patch_size = (10, 10)  # Define the size of the patch\n",
    "initial_patch = create_initial_patch(size=patch_size, device=device)\n",
    "\n",
    "# Train the patch\n",
    "trained_patch = train_patch(model, initial_patch, trainloader, device=device, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "188b442c-c7cd-4d49-b09e-9608690173e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save or visualize the trained patch here \n",
    "def visualize_patch(patch):\n",
    "    \"\"\"Visualize the adversarial patch.\"\"\"\n",
    "    # Move patch to CPU and convert to numpy\n",
    "    patch_np = patch.detach().cpu().numpy()\n",
    "    \n",
    "    # Reshape and normalize the patch for visualization\n",
    "    patch_np = np.transpose(patch_np, (1, 2, 0))  # Change from (C, H, W) to (H, W, C)\n",
    "    patch_np = (patch_np - patch_np.min()) / (patch_np.max() - patch_np.min())  # Normalize to [0, 1]\n",
    "\n",
    "    plt.imshow(patch_np)\n",
    "    plt.title(\"Adversarial Patch\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the trained patch\n",
    "visualize_patch(trained_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cae6f85-89a1-4876-a9db-f2f0759f0da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model after the attack: 47.41 %\n",
      "Predicted label distribution: [ 351 1343  905 2179  167 2507  255 1335   30  928]\n"
     ]
    }
   ],
   "source": [
    "def test_model_with_patch(model, patch, test_loader, device):\n",
    "    \"\"\"Evaluate the model on the test set with the adversarial patch applied.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_distribution = torch.zeros(len(test_loader.dataset.classes), dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            patched_images = apply_patch(images, patch)  # Apply the patch\n",
    "\n",
    "            outputs = model(patched_images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for label in predicted:\n",
    "                predicted_distribution[label] += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, predicted_distribution\n",
    "\n",
    "accuracy_after_attack, label_distribution = test_model_with_patch(model, trained_patch, testloader, device)\n",
    "\n",
    "print('Accuracy of the model after the attack: {:.2f} %'.format(accuracy_after_attack))\n",
    "print('Predicted label distribution:', label_distribution.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6680218-b7b9-43ae-8faa-247b79ea20f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7360\n",
      "Epoch [2/20], Loss: 0.3309\n",
      "Epoch [3/20], Loss: 0.2583\n",
      "Epoch [4/20], Loss: 0.2141\n",
      "Epoch [5/20], Loss: 0.1872\n",
      "Epoch [6/20], Loss: 0.1659\n",
      "Epoch [7/20], Loss: 0.1503\n",
      "Epoch [8/20], Loss: 0.1313\n",
      "Epoch [9/20], Loss: 0.1162\n",
      "Epoch [10/20], Loss: 0.1035\n",
      "Epoch [11/20], Loss: 0.0946\n",
      "Epoch [12/20], Loss: 0.0876\n",
      "Epoch [13/20], Loss: 0.0787\n",
      "Epoch [14/20], Loss: 0.0682\n",
      "Epoch [15/20], Loss: 0.0602\n",
      "Epoch [16/20], Loss: 0.0543\n",
      "Epoch [17/20], Loss: 0.0499\n",
      "Epoch [18/20], Loss: 0.0460\n",
      "Epoch [19/20], Loss: 0.0430\n",
      "Epoch [20/20], Loss: 0.0410\n"
     ]
    }
   ],
   "source": [
    "def train_patch_targeted(model, patch, train_loader, target_class, device='cpu', epochs=5):\n",
    "    \"\"\"Train the patch to perform a targeted attack.\"\"\"\n",
    "    optimizer = torch.optim.Adam([patch], lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, _ in train_loader:\n",
    "            target_labels = torch.full((len(images),), target_class, device=device)\n",
    "            images = images.to(device)\n",
    "            patched_images = apply_patch(images, patch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(patched_images)\n",
    "            loss = criterion(outputs, target_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return patch\n",
    "\n",
    "patch_size = (10, 10)  # Define the size of the patch\n",
    "initial_patch = create_initial_patch(size=patch_size, device=device)\n",
    "target_class = 5  # Example: 0 for the first class in CIFAR-10\n",
    "trained_patch_targeted = train_patch_targeted(model, initial_patch, trainloader, target_class, device=device, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c05a40f-21ba-4de1-825a-344f72b6fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model after the attack: 10.69 %\n",
      "Predicted label distribution: [   3   10    1    3   11 9931    1    7    4   29]\n"
     ]
    }
   ],
   "source": [
    "accuracy_after_attack, label_distribution = test_model_with_patch(model, trained_patch_targeted, testloader, device)\n",
    "\n",
    "print('Accuracy of the model after the attack: {:.2f} %'.format(accuracy_after_attack))\n",
    "print('Predicted label distribution:', label_distribution.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92895363-4c38-42cc-a78b-3ee81b9c86e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
