{"cells":[{"cell_type":"markdown","metadata":{"id":"3_7rNZfIf301"},"source":["## Installing the relevant libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install tqdm\n","!pip install transformers datasets\n","!pip install torchinfo\n"]},{"cell_type":"markdown","metadata":{},"source":["## Importing the packages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:20:07.886307Z","iopub.status.busy":"2021-11-18T06:20:07.885822Z","iopub.status.idle":"2021-11-18T06:20:14.242554Z","shell.execute_reply":"2021-11-18T06:20:14.241802Z","shell.execute_reply.started":"2021-11-18T06:20:07.88627Z"},"id":"6MCTZFkFw6i6","outputId":"a4ab4331-5fda-4290-a218-baa9bb788b37","trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from datasets import Array3D, ClassLabel, Features, load_dataset\n","from matplotlib import pyplot\n","from numpy import inf\n","from sklearn.utils.class_weight import compute_class_weight\n","from torchinfo import summary\n","from tqdm import tqdm\n","from transformers import AdamW, ViTFeatureExtractor, ViTModel"]},{"cell_type":"markdown","metadata":{},"source":["## Downloading the data and preparing the train, validation and test datasets\n","\n","Here we import a small portion of CIFAR-10 dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XC9HqG5u750_","outputId":"0319f84f-32c6-49af-8bb8-5872ac4083fb","trusted":true},"outputs":[],"source":["# load cifar10 \n","train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])\n","# split up training into training + validation\n","splits = train_ds.train_test_split(test_size=0.1)\n","train_ds = splits['train']\n","val_ds = splits['test']"]},{"cell_type":"markdown","metadata":{"id":"AZXEXiozxB-4"},"source":["## Feature Extractor\n","\n","We are using `ViTFeatureExtractor`. This feature extractor will  resize/rescale the images to the same resolution (224x224) and normalize them across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:20:14.244196Z","iopub.status.busy":"2021-11-18T06:20:14.24384Z","iopub.status.idle":"2021-11-18T06:20:15.016112Z","shell.execute_reply":"2021-11-18T06:20:15.015393Z","shell.execute_reply.started":"2021-11-18T06:20:14.244159Z"},"id":"NjNs_qptzFSl","trusted":true},"outputs":[],"source":["feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:21:35.454321Z","iopub.status.busy":"2021-11-18T06:21:35.454059Z","iopub.status.idle":"2021-11-18T06:21:35.703632Z","shell.execute_reply":"2021-11-18T06:21:35.702829Z","shell.execute_reply.started":"2021-11-18T06:21:35.454285Z"},"trusted":true},"outputs":[],"source":["chk = train_ds[67]\n","a = np.array(chk['img'])\n","print(a.shape)\n","cats = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","\n","print(cats[chk['label']])\n","pyplot.imshow(a, cmap=pyplot.get_cmap('gray'))"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess Images\n","The preprocess_images function is used to process each image in the dataset. These processed images will be fed to the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:21:35.705442Z","iopub.status.busy":"2021-11-18T06:21:35.705158Z","iopub.status.idle":"2021-11-18T06:21:35.711128Z","shell.execute_reply":"2021-11-18T06:21:35.710422Z","shell.execute_reply.started":"2021-11-18T06:21:35.705405Z"},"id":"M8fcwD0OxEHP","trusted":true},"outputs":[],"source":["def preprocess_images(examples):\n","    # get batch of images\n","    images = examples['img']\n","    # convert to list of NumPy arrays of shape (C, H, W)\n","    images = [np.array(image, dtype=np.uint8) for image in images]\n","    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n","    # preprocess and add pixel_values\n","    inputs = feature_extractor(images=images)\n","    examples['pixel_values'] = inputs['pixel_values']\n","    return examples"]},{"cell_type":"markdown","metadata":{"id":"ywJ0iarBhXpc"},"source":["HuggingFace Datasets .map(function, batched=True) functionality is used apply the preprocess_images function on every item in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:21:35.713351Z","iopub.status.busy":"2021-11-18T06:21:35.712789Z","iopub.status.idle":"2021-11-18T06:22:36.259748Z","shell.execute_reply":"2021-11-18T06:22:36.258918Z","shell.execute_reply.started":"2021-11-18T06:21:35.713307Z"},"id":"HimhtFgIxEfg","outputId":"4eaaab10-e38b-49eb-ad80-ac508d6ba878","trusted":true},"outputs":[],"source":["# we need to define the features ourselves as both the img and pixel_values have a 3D shape \n","features = Features({\n","    'label': ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']),\n","    'img': Array3D(dtype=\"int64\", shape=(3,32,32)),\n","    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n","})\n","\n","preprocessed_train_ds = train_ds.map(preprocess_images, batched=True, features=features)\n","preprocessed_val_ds = val_ds.map(preprocess_images, batched=True, features=features)\n","preprocessed_test_ds = test_ds.map(preprocess_images, batched=True, features=features)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:22:36.261417Z","iopub.status.busy":"2021-11-18T06:22:36.26115Z","iopub.status.idle":"2021-11-18T06:22:36.272311Z","shell.execute_reply":"2021-11-18T06:22:36.270016Z","shell.execute_reply.started":"2021-11-18T06:22:36.261382Z"},"id":"rwYeg3vy4g-U","trusted":true},"outputs":[],"source":["# set format to PyTorch\n","preprocessed_train_ds.set_format('torch', columns=['pixel_values', 'label'])\n","preprocessed_val_ds.set_format('torch', columns=['pixel_values', 'label'])\n","preprocessed_test_ds.set_format('torch', columns=['pixel_values', 'label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:22:36.274095Z","iopub.status.busy":"2021-11-18T06:22:36.273263Z","iopub.status.idle":"2021-11-18T06:22:44.456583Z","shell.execute_reply":"2021-11-18T06:22:44.453807Z","shell.execute_reply.started":"2021-11-18T06:22:36.274056Z"},"trusted":true},"outputs":[],"source":["preprocessed_train_ds"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing the train, validation and test data loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:22:44.459931Z","iopub.status.busy":"2021-11-18T06:22:44.459573Z","iopub.status.idle":"2021-11-18T06:22:44.814144Z","shell.execute_reply":"2021-11-18T06:22:44.813173Z","shell.execute_reply.started":"2021-11-18T06:22:44.45989Z"},"id":"6QgzR_vnIWbO","trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# create dataloaders\n","train_batch_size = 10\n","eval_batch_size = 10\n","train_dataloader = torch.utils.data.DataLoader(preprocessed_train_ds, batch_size=train_batch_size, shuffle=True, num_workers=2)\n","val_dataloader = torch.utils.data.DataLoader(preprocessed_val_ds, batch_size=eval_batch_size, num_workers=2)\n","test_dataloader = torch.utils.data.DataLoader(preprocessed_test_ds, batch_size=eval_batch_size, num_workers=2)\n","batch = next(iter(train_dataloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLCEbNfcbhk6","outputId":"82e0cb33-ffc6-4720-9699-83735082c77f","trusted":true},"outputs":[],"source":["batch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:22:44.897889Z","iopub.status.busy":"2021-11-18T06:22:44.897646Z","iopub.status.idle":"2021-11-18T06:22:44.905789Z","shell.execute_reply":"2021-11-18T06:22:44.905079Z","shell.execute_reply.started":"2021-11-18T06:22:44.897857Z"},"id":"NxROptl3Ifch","trusted":true},"outputs":[],"source":["assert batch['pixel_values'].shape == (train_batch_size, 3, 224, 224)\n","assert batch['label'].shape == (train_batch_size,)"]},{"cell_type":"markdown","metadata":{"id":"JDefWdsUxEsK"},"source":["## Define the model\n","\n","Here we are using Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.\n","\n","Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n","\n","https://huggingface.co/google/vit-base-patch16-224-in21k"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:22:44.913067Z","iopub.status.busy":"2021-11-18T06:22:44.910756Z","iopub.status.idle":"2021-11-18T06:23:06.505186Z","shell.execute_reply":"2021-11-18T06:23:06.504433Z","shell.execute_reply.started":"2021-11-18T06:22:44.913029Z"},"trusted":true},"outputs":[],"source":["vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","for param in vit_model.parameters():\n","      param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:23:06.506651Z","iopub.status.busy":"2021-11-18T06:23:06.506401Z","iopub.status.idle":"2021-11-18T06:23:06.513712Z","shell.execute_reply":"2021-11-18T06:23:06.512792Z","shell.execute_reply.started":"2021-11-18T06:23:06.506617Z"},"id":"w40w7H_kJSRq","trusted":true},"outputs":[],"source":["class ViTForImageClassification(nn.Module):\n","    def __init__(self, num_labels=10):\n","        super(ViTForImageClassification, self).__init__()\n","        self.vit = vit_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n","        self.num_labels = num_labels\n","\n","    def forward(self, pixel_values):\n","        outputs = self.vit(pixel_values=pixel_values)\n","        output = self.dropout(outputs.last_hidden_state[:,0])\n","        logits = self.classifier(output)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:23:06.515604Z","iopub.status.busy":"2021-11-18T06:23:06.515058Z","iopub.status.idle":"2021-11-18T06:23:11.382656Z","shell.execute_reply":"2021-11-18T06:23:11.381877Z","shell.execute_reply.started":"2021-11-18T06:23:06.515564Z"},"trusted":true},"outputs":[],"source":["model = ViTForImageClassification()\n","model = model.to(device)\n","# summary(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Computing the class weights to handle the data imbalance"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#compute the class weights\n","class_wts = compute_class_weight(\"balanced\", np.unique(preprocessed_train_ds['label']), preprocessed_train_ds['label'].tolist())\n","# print(class_wts)\n","\n","# # convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","# # loss function\n","cross_entropy = nn.CrossEntropyLoss(weight=weights)"]},{"cell_type":"markdown","metadata":{},"source":["## Setting the Optimizer and Epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:23:11.425096Z","iopub.status.busy":"2021-11-18T06:23:11.423885Z","iopub.status.idle":"2021-11-18T06:23:11.430892Z","shell.execute_reply":"2021-11-18T06:23:11.430146Z","shell.execute_reply.started":"2021-11-18T06:23:11.425057Z"},"trusted":true},"outputs":[],"source":["# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)\n","\n","# number of training epochs\n","epochs = 50"]},{"cell_type":"markdown","metadata":{},"source":["## Model Train function\n","It is to train the train dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:23:11.434021Z","iopub.status.busy":"2021-11-18T06:23:11.433783Z","iopub.status.idle":"2021-11-18T06:23:11.444401Z","shell.execute_reply":"2021-11-18T06:23:11.443349Z","shell.execute_reply.started":"2021-11-18T06:23:11.433995Z"}},"outputs":[],"source":["# function to train the model\n","def train():\n","  \n","    model.train()\n","    total_loss = 0\n","\n","    # empty list to save model predictions\n","    total_preds=[]\n","\n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step,    len(train_dataloader)))\n","        \n","        # push the batch to gpu\n","        lbl, pix = batch.items()\n","        lbl, pix = lbl[1].to(device), pix[1].to(device)\n","        \n","        # get model predictions for the current batch\n","        preds = model(pix)\n","       \n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, lbl)\n","        \n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","        \n","        # backward pass to calculate the gradients\n","        loss.backward()\n","        \n","        # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        \n","        # update parameters\n","        optimizer.step()\n","        \n","        # clear calculated gradients\n","        optimizer.zero_grad()  \n","        preds=preds.detach().cpu().numpy()\n","        \n","        # append the model predictions\n","        total_preds.append(preds)\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","  \n","    total_preds  = np.concatenate(total_preds, axis=0)\n","    \n","    #returns the loss and predictions\n","    return avg_loss, total_preds\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Eval function\n","It is to evaluate the validation dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:23:11.446313Z","iopub.status.busy":"2021-11-18T06:23:11.44558Z","iopub.status.idle":"2021-11-18T06:23:11.456541Z","shell.execute_reply":"2021-11-18T06:23:11.455625Z","shell.execute_reply.started":"2021-11-18T06:23:11.446275Z"},"trusted":true},"outputs":[],"source":["def eval():\n","    total_loss = 0\n","    model.eval() # prep model for evaluation\n","    for step,batch in enumerate(val_dataloader):\n","        lbl, pix = batch.items()\n","        lbl, pix = lbl[1].to(device), pix[1].to(device)\n","        \n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        preds = model(pix)\n","        # calculate the loss\n","        loss = cross_entropy(preds, lbl)\n","        total_loss += loss.item()\n","    \n","    return total_loss / len(val_dataloader)"]},{"cell_type":"markdown","metadata":{},"source":["## Training the model \n","\n","Training and checking the training loss on train data loader and validation loss on validation data loader "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["min_loss = inf\n","es = 0\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    # Train model\n","    train_loss, _ = train()\n","    val_loss = eval()\n","    \n","    # Early Stopping\n","    if val_loss < min_loss:\n","        min_loss = val_loss\n","        es = 0\n","    else:\n","        es += 1\n","        if es > 4:\n","            print(\"Early stopping with train_loss: \", train_loss, \"and val_loss for this epoch: \", val_loss, \"...\")\n","            break\n","    \n","    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    print(f'\\n Training Loss: {train_loss:.3f}')\n","    print(f'\\n Validation Loss: {val_loss:.3f}')"]},{"cell_type":"markdown","metadata":{},"source":["## Save the model weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:31:08.036255Z","iopub.status.busy":"2021-11-18T06:31:08.035717Z","iopub.status.idle":"2021-11-18T06:31:08.040081Z","shell.execute_reply":"2021-11-18T06:31:08.039388Z","shell.execute_reply.started":"2021-11-18T06:31:08.036207Z"},"trusted":true},"outputs":[],"source":["# torch.save(model.state_dict(), '/kaggle/working/model')"]},{"cell_type":"markdown","metadata":{},"source":["## Load the model weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:31:08.041925Z","iopub.status.busy":"2021-11-18T06:31:08.041439Z","iopub.status.idle":"2021-11-18T06:31:08.051416Z","shell.execute_reply":"2021-11-18T06:31:08.050684Z","shell.execute_reply.started":"2021-11-18T06:31:08.041888Z"},"trusted":true},"outputs":[],"source":["# model = ViTForImageClassification()\n","# model.load_state_dict(torch.load('/kaggle/working/model'), strict=False)\n","\n","# # push the model to GPU\n","# model = model.to(device)\n","# summary(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the model on Test DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:39:39.308185Z","iopub.status.busy":"2021-11-18T06:39:39.307873Z","iopub.status.idle":"2021-11-18T06:39:53.461036Z","shell.execute_reply":"2021-11-18T06:39:53.460219Z","shell.execute_reply.started":"2021-11-18T06:39:39.308154Z"},"id":"xjoneN5EK7Ol","trusted":true},"outputs":[],"source":["def eval():\n","    model.eval()\n","    y_pred = []\n","    y_true = []\n","    with torch.no_grad():\n","        for step, batch in tqdm(enumerate(test_dataloader), total = len(test_dataloader)):\n","            lbl, pix = batch.items()\n","            lbl, pix = lbl[1].to(device), pix[1].to(device)\n","\n","            outputs = model(pix)\n","            outputs = torch.argmax(outputs, axis=1)\n","            y_pred.extend(outputs.cpu().detach().numpy())\n","            y_true.extend(lbl.cpu().detach().numpy())\n","            \n","    return y_pred, y_true\n","\n","y_pred, y_true = eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-18T06:39:57.247325Z","iopub.status.busy":"2021-11-18T06:39:57.247019Z","iopub.status.idle":"2021-11-18T06:39:57.255083Z","shell.execute_reply":"2021-11-18T06:39:57.254334Z","shell.execute_reply.started":"2021-11-18T06:39:57.247283Z"},"trusted":true},"outputs":[],"source":["correct = np.array(y_pred) == np.array(y_true)\n","accuracy = correct.sum() / len(correct)\n","print(\"Accuracy of the model\", accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30145,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
